{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.file_ops import add_filename_timestamp\n",
    "import shutil\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Setup\n",
    "\n",
    "GUI Ideas:\n",
    "- [ ] Use GUI to select files\n",
    "- [ ] Option to select .csv containing partnumbers to be removed\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\104092\\\\OneDrive - Grundfos\\\\Documents\\\\git\\\\grundfos-express-tools\\\\bronze impeller removal\\\\output files\\\\2022-08-10_14-24-36 VLSEbom.xlsx'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDir = r\"C:\\Users\\104092\\OneDrive - Grundfos\\Documents\\git\\grundfos-express-tools\\bronze impeller removal\\input files\"\n",
    "file = \"VLSEbom.xlsx\"\n",
    "filePath = os.path.join(myDir, file)\n",
    "\n",
    "# Create working copy\n",
    "completed_dir = r\"C:\\Users\\104092\\OneDrive - Grundfos\\Documents\\git\\grundfos-express-tools\\bronze impeller removal\\output files\"\n",
    "working_copy = os.path.join(completed_dir, add_filename_timestamp(file))\n",
    "shutil.copyfile(filePath, working_copy) # creates working copy to leave original file untouched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of partnumbers to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pns = [\n",
    "    96699290, 97775274, 96699299, 97775277, 96778078,\n",
    "    97780992, 96699305, 96769184, 97778033, 96769190,\n",
    "    96769205, 97778039, 96769256, 96896891, 96769259,\n",
    "    96769271, 97780970, 96769280, 97780973\n",
    "]\n",
    "\n",
    "str_list = [str(numstr) for numstr in list_of_pns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating justification note to add to data \n",
    "- [ ] Make a GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reason/Justification for removal\n",
    "timeStamp = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "reason = input(\"Reason for removal: \")\n",
    "authority = input(\"Who authorized/requested this change? \")\n",
    "\n",
    "removal_note = timeStamp + \" \" + reason + \" per \" + authority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in excel to dataframe\n",
    "- [ ] Add to GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize based on PST template used. \n",
    "psd_startrow = 5 # Corresponds to row number containing \"second\" column headers\n",
    "psd_startcol = 0\n",
    "\n",
    "# Read in Excel sheet to a dataframe\n",
    "sheetname = \"Impeller\"\n",
    "psd_data = pd.read_excel(working_copy, sheet_name=sheetname, header=psd_startrow, dtype={'BOM': str}, skipfooter=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and separating data\n",
    "[ ] Add column names for grouping to GUI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_then_separate_by(list_of_cols: list, pn_col: str):\n",
    "    \"\"\"list of cols are grouping categories. pn_col is the column that contains pns to find matches on.\"\"\"\n",
    "    groups = psd_data.groupby(list_of_cols) # Had to play around with this to get the right groupings\n",
    "\n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "        if any(frame[pn_col].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            df_list_to_remove.append(frame)     # Need to add this sub-group to a \"removed dataframe\"\n",
    "        else:\n",
    "            df_list_to_keep.append(frame)       # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    removals = pd.concat(df_list_to_remove)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "\n",
    "    return removals, keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matching partnumber's from list above, and then capture \"process variants\"\n",
    "group_by_columns = [\"Model\", \"Price ID\"]  # For just one column, use [\"Col 1\", ]\n",
    "removals, keep = group_then_separate_by(group_by_columns, \"BOM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe formatting.\n",
    "removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = \"\" # This removes the [START] if the first PSD row is flagged to be removed\n",
    "\n",
    "# Insert removal justification note to top of removal dataframe\n",
    "new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "removals = pd.concat([new_row, removals[:]]).reset_index() # Concatenate new_row with df\n",
    "\n",
    "column_list = keep.columns\n",
    "removals = removals[column_list] # Reorder columns to match the 2 dataframe columns prior to concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks for any misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't miss any partnumbers. Good to go\n"
     ]
    }
   ],
   "source": [
    "# Check for any misses before writing. Should return empty dataframe\n",
    "if (len(keep[keep['BOM'].isin(str_list)])) == 0:\n",
    "    print(\"Didn't miss any partnumbers. Good to go\")\n",
    "else:\n",
    "    print(\"For some reason, the following entries were missed\")\n",
    "    print(keep[keep['BOM'].isin(str_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing excel file with appropriate formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of PSD tab with formatting\n",
    "wb = load_workbook(working_copy)\n",
    "ws = wb['Impeller']\n",
    "tabName = sheetname + \" No Bronze\"\n",
    "wb.copy_worksheet(ws).title = tabName\n",
    "\n",
    "# Find row number to append removals and insert reason why they were removed \n",
    "original_end_location = len(psd_data) + psd_startrow          # This is the last row of the original PSD (0-indexed) \n",
    "append_location = original_end_location - len(removals) + 5   # This is the new location/row where we will append the removals to bottom of PSD\n",
    "\n",
    "# Edit New Tab to clear cells with old data after first [END] value is reached\n",
    "num_rows = len(keep)                          # Finds where [END] will be on PSD\n",
    "after_end_row = num_rows + psd_startrow + 1   # Calculates where [END] row + 1 will be in the PSD\n",
    "\n",
    "ws_modified = wb[tabName]\n",
    "ws_modified.delete_rows(after_end_row-1, len(removals)-1)   # clears the rows containing old data\n",
    "\n",
    "# Fix formatting\n",
    "from openpyxl.styles import PatternFill\n",
    "for rows in ws_modified.iter_rows(min_row=append_location+2, max_row=append_location+2, min_col=1, max_col=40):\n",
    "    for cell in rows:\n",
    "        cell.fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "\n",
    "wb.save(working_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write resulting dataframes to new sheet in PSD with changes.\n",
    "with pd.ExcelWriter(working_copy, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:  \n",
    "    keep.to_excel(writer, sheet_name=tabName, index=False, startrow=psd_startrow, startcol=psd_startcol)\n",
    "    removals.to_excel(writer, sheet_name=tabName, index=False, header=False, startrow=append_location+1, startcol=psd_startcol)\n",
    "\n",
    "# Remove original tab, replace with new one?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48fa2eccdebaf1ff24b5e55dfacc47ca863d8022685fa20f5e1e5031fac11810"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
